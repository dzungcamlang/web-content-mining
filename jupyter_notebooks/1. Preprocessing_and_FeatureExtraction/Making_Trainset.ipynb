{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "#nltk.download()\n",
    "import codecs\n",
    "import os\n",
    "import nltk\n",
    "#import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8') # np.savetxt(train_dataset_path, final_vector, fmt='%s', delimiter=\",\")을 원활히 해주기 위해 그리고 이것 때문에 print 결과가 안나온다.\n",
    "from html_preprocessing import *\n",
    "from publisher_rule import labeling_data\n",
    "from leaf_information import *\n",
    "\n",
    "global leafs_nseq_info, leafs_seq_info, leafs_nested_level, publisher, final_vector\n",
    "global tagName_dic_1hot_list, tagAttr_dic_1hot_list, tagString_dic_1hot_list\n",
    "leafs_nseq_info = []\n",
    "leafs_seq_info = []\n",
    "leafs_nested_level = []\n",
    "final_vector = []\n",
    "tagName_dic_1hot_list = []\n",
    "tagAttr_dic_1hot_list = []\n",
    "tagString_dic_1hot_list = []\n",
    "\n",
    "global n_token_min, n_token_max, n_int_min, n_int_max\n",
    "global n_sibling_min, n_sibling_max, n_parent_min, n_parent_max, n_element_min, n_element_max\n",
    "n_token_min = 50\n",
    "n_token_max = 0\n",
    "n_int_min = 50\n",
    "n_int_max = 0\n",
    "n_sibling_min = 50\n",
    "n_sibling_max = 0\n",
    "n_parent_min = 50\n",
    "n_parent_max = 0\n",
    "n_element_min = 50\n",
    "n_element_max = 0\n",
    "\n",
    "global count\n",
    "count = 1\n",
    "\n",
    "\n",
    "\n",
    "# extracting number of the same tags\n",
    "# soup때문에 main에 상주\n",
    "def n_of_sametag(tag): \n",
    "    return len(list(soup.find_all(tag.name)))\n",
    "\n",
    "def distance_to_h1(tag): # ex input: (nested tag list, 'body') \n",
    "    h1 = len(list(soup.h1.previous_elements))\n",
    "    current_tag = len(list(tag.previous_elements))\n",
    "    return abs(h1-current_tag)\n",
    "\n",
    "def rescale_bounded(min_value, max_value, x):\n",
    "    return ((2*x - max_value - min_value)/float(max_value - min_value)) if float(max_value - min_value)!=0 else 0\n",
    "    \n",
    "def find_min_max(tag):   \n",
    "    global n_token_min, n_token_max, n_int_min, n_int_max\n",
    "    global n_sibling_min, n_sibling_max, n_parent_min, n_parent_max, n_element_min, n_element_max\n",
    "    if tag.string!=None and tag.string!=' ' and len(list(tag.children))==1 and len(list(tag.descendants))==1:\n",
    "        \n",
    "        n_token_temp =  n_of_tok(tag)\n",
    "        if n_token_min > n_token_temp: \n",
    "            n_token_min = n_token_temp\n",
    "        if n_token_max < n_token_temp: \n",
    "            n_token_max = n_token_temp\n",
    "                \n",
    "        n_int_temp = n_of_int(tag)\n",
    "        if n_int_min > n_int_temp: \n",
    "            n_int_min = n_int_temp\n",
    "        if n_int_max < n_int_temp: \n",
    "            n_int_max = n_int_temp\n",
    "        \n",
    "        n_sibling_temp = n_total_sib(tag)\n",
    "        if n_sibling_min > n_sibling_temp: \n",
    "            n_sibling_min = n_sibling_temp\n",
    "        if n_sibling_max < n_sibling_temp: \n",
    "            n_sibling_max = n_sibling_temp        \n",
    "\n",
    "        n_parent_temp = n_of_parents(tag)\n",
    "        if n_parent_min > n_parent_temp: \n",
    "            n_parent_min = n_parent_temp\n",
    "        if n_parent_max < n_parent_temp: \n",
    "            n_parent_max = n_parent_temp            \n",
    "            \n",
    "        n_element_temp = (len(list(tag.previous_elements)))\n",
    "        if n_element_min > n_element_temp: \n",
    "            n_element_min = n_element_temp\n",
    "        if n_element_max < n_element_temp: \n",
    "            n_element_max = n_element_temp \n",
    "\n",
    "               \n",
    "def find_leaf_node(tag):\n",
    "    global leafs_nseq_info, leafs_seq_info, leafs_nested_level, publisher, final_vector, count\n",
    "    if tag.string!=None and tag.string!=' ' and len(list(tag.children))==1 and len(list(tag.descendants))==1:\n",
    "        tagli = extract_nested_level(tag).encode('utf-8')\n",
    "        \n",
    "        if filtering_tagname(tag, tagli)==1 and filtering_tagattrs(tag.attrs.values())==1: # if pass, continue \n",
    "            if filtering_tagattrs(tag.parent.attrs.values())==1 and filtering_tagattrs(tag.parent.parent.attrs.values())==1:\n",
    "                if filtering_tagattrs(tag.parent.parent.parent.attrs.values())==1 and filtering_etc(tag)==1:\n",
    "                    \n",
    "                    #if filtering_full_hyperlink(tag)==1:\n",
    "                        \n",
    "                    #list = [ # of tokens , # of integers, diff, ratio, # of parents, # of siblings, # of sametag ]\n",
    "                    nonseq_list = [\n",
    "                        \n",
    "                        # Label\n",
    "                        (labeling_data(tag, publisher)), \n",
    "                            \n",
    "                        \n",
    "                        # Continuous features\n",
    "                        rescale_bounded(n_token_min, n_token_max, n_of_tok(tag)), \n",
    "                        #rescale_bounded(n_int_min, n_int_max, n_of_int(tag)),\n",
    "                        rescale_bounded(n_element_min, n_element_max, len(list(tag.previous_elements))),  #(len(list(tag.next_elements))),\n",
    "                        rescale_bounded(n_parent_min, n_parent_max, n_of_parents(tag)), \n",
    "                        rescale_bounded(n_sibling_min, n_sibling_max, n_total_sib(tag)), #(n_next_sib(tag)), (n_pre_sib(tag)),    \n",
    "                        \n",
    "                        \n",
    "                        # Binary features\n",
    "                        (is_comma_in_string(tag)), \n",
    "                        (ratio(n_of_int(tag), n_of_tok(tag))),\n",
    "                        (ratio(n_of_uppercase(tag), n_of_tok(tag))),\n",
    "                        (is_date(tag)), #(distance_to_h1(tag)), # for date class\n",
    "                        (is_hyper_bothends(tag)),\n",
    "                        \n",
    "                        \n",
    "                        # Current_TagName-Dic-based binary features\n",
    "                         (is_tag(tag, 'h1')), (is_tag(tag, 'p')), #(is_tag(tag, 'span')),      \n",
    "                         (is_tag(tag, 'time')), (is_tag(tag, 'span')),\n",
    "\n",
    "#                         (allli_is_tag(tagli, 'article')), (allli_is_tag(tagli, 'header')),\n",
    "#                         (allli_is_tag(tagli, 'section')), (allli_is_tag(tagli, 'li')), #(allli_is_tag(tagli, 'ol')),\n",
    "#                         (allli_is_tag(tagli, 'ui')), \n",
    "#                         (allli_is_tag(tagli, 'figure')), \n",
    "#                         (allli_is_tag(tagli, 'select')), (allli_is_tag(tagli, 'form')),\n",
    "#                         #(allli_is_tag(tagli, 'h2')),(allli_is_tag(tagli, 'h3')),(allli_is_tag(tagli, 'h4')), \n",
    "#                         (allli_is_tag(tagli, 'p'))\n",
    "                    ]\n",
    "                    leafs_nseq_info.append(nonseq_list) \n",
    "\n",
    "                    \n",
    "                    # All_Parrents_TagName-Dic-based binary features\n",
    "                    tagName_dic_1hot_list.append(make_1hot_features(tag, 'tag_name'))\n",
    "                    \n",
    "                    # TagAttrs-Dic-based binary features\n",
    "                    tagAttr_dic_1hot_list.append(make_1hot_features(tag, 'tag_attr'))\n",
    "                    \n",
    "                    # TagString-Dic-based binary features\n",
    "                    tagString_dic_1hot_list.append(make_1hot_features(tag, 'tag_string'))\n",
    "                \n",
    "                \n",
    "                \n",
    "                    #seq_list = [ labeling_data(tag), extract_tag_name_attrs(tag) ]\n",
    "                    temp_list = []\n",
    "                    temp_list.append(labeling_data(tag, publisher))\n",
    "                    seq_list = temp_list + extract_tag_name_attrs(tag)\n",
    "                    leafs_seq_info.append(seq_list)\n",
    "\n",
    "                    leafs_nested_level.append(extract_nested_level(tag).encode('utf-8')) # 'ascii' codec can't decode byte 0xc2 in position 23: ordinal not in range(128) 에러 발생 -> utf-8로 encoding\n",
    "                    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\Office\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Users\\Office\\Anaconda2\\lib\\site-packages\\dateutil\\parser.py:601: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  elif res.tzname and res.tzname in time.tzname:\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#                               Main Function\n",
    "#==============================================================================\n",
    "# 윈도우는 경로 '\\\\' 이지만, 리눅스는 '/'이다.\n",
    "# 윈도우에서 파일 열고있는 상태에서 (해당파일에 접근하는) 프로그램 실행하면 오류난다.\n",
    "# http://pythoncentral.io/how-to-traverse-a-directory-tree-in-python-guide-to-os-walk/\n",
    "#dataset_path = './article_dataset'\n",
    "full_path = 'C:\\\\Users\\\\Office\\\\Desktop\\\\Python Workspace\\\\Tag Classification Project\\\\'\n",
    "#dataset_path = '.\\\\Dataset\\\\test_ori'\n",
    "dataset_path = full_path+'Dataset\\\\data_unlabeled'\n",
    "#dataset_path = full_path+'Dataset\\\\test'\n",
    "#dataset_path = full_path+'Dataset\\\\testest'\n",
    "\n",
    "#이중 폴더 속에 있는 파일들을 Load하기 위한 이중 for문\n",
    "for dirName, subdirList, fileList in os.walk(dataset_path):\n",
    "    #print('Found directory: %s' % dirName)\n",
    "    for fname in fileList:\n",
    "\n",
    "        # 파일의 절대 경로를 만들어 주기 위한 코드\n",
    "        file_path = dirName+'\\\\'+fname # ex: ./test_dataset/Telegraph/Google-2010-Telegraph-20160706164841234.html\n",
    "        temp_str = dirName.split('\\\\')\n",
    "        publisher = temp_str[-1] # 현재 publisher 폴더이름\n",
    "        \n",
    "        \n",
    "        ### a single html file load & preprocessing\n",
    "        html_file = codecs.open(file_path, 'r')\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "        soup = html_preprocessing(soup)\n",
    "        \n",
    "        \n",
    "        ### Calculate min/max values for Rescale bounded continuous features\n",
    "        soup.find_all(find_min_max)\n",
    " \n",
    "\n",
    "        ### Make training data set\n",
    "        leaf_nodes = soup.find_all(find_leaf_node)\n",
    "        \n",
    "        ## 모든 vector들의 dim의 통일해주기 위한 코드\n",
    "        final_vector = leafs_nseq_info\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx, vector in enumerate(final_vector):\n",
    "            vector += tagName_dic_1hot_list[idx][:]\n",
    "            vector += tagAttr_dic_1hot_list[idx][:]\n",
    "            vector += tagString_dic_1hot_list[idx][:]\n",
    "            vector += leafs_seq_info[idx][1:] # list+list\n",
    "            vector.append(str(leaf_nodes[idx]).replace(\",\", \"(c)\")) # list+string\n",
    "            vector.append(leafs_nested_level[idx])\n",
    "    \n",
    "        \n",
    "        #final_vector.insert(0, [\"class\",\"# token\",\"# int\",\"dklf\"]) # column name\n",
    "\n",
    "        \n",
    "        ### Storing to CSV file \n",
    "        #train_dataset_path = './labeled_dataset'\n",
    "        #train_dataset_path = './labeled_dataset'\n",
    "        \n",
    "        train_dataset_path = full_path+'Dataset\\\\data_labeled'\n",
    "        #train_dataset_path = full_path+'Dataset\\\\'\n",
    "        \n",
    "        \n",
    "        \n",
    "        file_name = fname.replace(\"html\", \"csv\")\n",
    "        \n",
    "        train_dataset_path = train_dataset_path+'\\\\'+publisher+'\\\\'+file_name\n",
    "        np.savetxt(train_dataset_path, final_vector, fmt='%s', delimiter=\",\")\n",
    "        \n",
    "        \n",
    "        ### Initialize global variables\n",
    "        leafs_nseq_info = []\n",
    "        leafs_seq_info = []\n",
    "        leafs_nested_level = []\n",
    "        final_vector = []\n",
    "        leaf_nodes = []\n",
    "        final_vector = []\n",
    "        tagName_dic_1hot_list = []\n",
    "        tagAttr_dic_1hot_list = []\n",
    "        tagString_dic_1hot_list = []\n",
    "        \n",
    "        n_token_min = 50\n",
    "        n_token_max = 0\n",
    "        n_int_min = 50\n",
    "        n_int_max = 0\n",
    "        n_sibling_min = 50\n",
    "        n_sibling_max = 0\n",
    "        n_parent_min = 50\n",
    "        n_parent_max = 0\n",
    "        n_element_min = 50\n",
    "        n_element_max = 0\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag_name_from_data = ['div', 'p', 'h1','a']\n",
    "        \n",
    "# # Load tag name dictionary\n",
    "# tag_name_dic = [row.rstrip('\\n') for row in open('tag_name_dic.txt')]\n",
    "# tag_name_dic_1hot_list = [0]*len(tag_name_dic)\n",
    "        \n",
    "# for i, word in enumerate(tag_name_dic):\n",
    "#     for data in tag_name_from_data:\n",
    "#         if word==data: \n",
    "#             tag_name_dic_1hot_list[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  ## 모든 vector들의 dim의 통일해주기 위한 코드\n",
    "# final_vector = leafs_nseq_info\n",
    "       \n",
    "# for idx, vector in enumerate(final_vector):\n",
    "#     vector+=leafs_seq_info[idx][1:] # list+list\n",
    "#     vector.append(str(leaf_nodes[idx]).replace(\",\", \"(c)\")) # list+string\n",
    "#     vector.append(leafs_nested_level[idx])\n",
    "    \n",
    "        \n",
    "#         #final_vector.insert(0, [\"class\",\"# token\",\"# int\",\"dklf\"]) # column name\n",
    "\n",
    "        \n",
    "#         ### Storing to CSV file \n",
    "#         #train_dataset_path = './labeled_dataset'\n",
    "#         #train_dataset_path = './labeled_dataset'\n",
    "        \n",
    "# train_dataset_path = full_path\n",
    "#     #train_dataset_path = full_path+'Dataset\\\\test\n",
    "        \n",
    "\n",
    "# train_dataset_path = train_dataset_path+'test1.csv'\n",
    "# np.savetxt(train_dataset_path, final_vector, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from dateutil.parser import parse\n",
    "# def date_string(string):\n",
    "#     dt = parse(string)\n",
    "#     return dt.strftime('%B/%d/%m/%Y')\n",
    "# #dt = parse('Crosswords')\n",
    "# #print(dt)\n",
    "# # datetime.datetime(2010, 2, 15, 0, 0)\n",
    "# #print(dt.strftime('%B/%d/%m/%Y'))\n",
    "# # 15/02/2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For Text\n",
    "# from bs4 import BeautifulSoup\n",
    "# def n_of_sametag(tag): \n",
    "#     return len(list(soup.find_all(tag.name)))\n",
    "\n",
    "# def distance_to_h1(tag): # ex input: (nested tag list, 'body') \n",
    "#     h1 = len(list(soup.h1.previous_elements))\n",
    "#     current_tag = len(list(tag.previous_elements))\n",
    "#     #print h1, current_tag\n",
    "#     return h1-current_tag\n",
    "    \n",
    "        \n",
    "# html_file =\"\"\"\n",
    "# <html>\n",
    "#     <style>\n",
    "#         <b1>The tree soup</b1>    \n",
    "#     </style>\n",
    "#     <head>\n",
    "#         <title>\n",
    "#             <h1>The 's story</h1>\n",
    "#         </title>\n",
    "#     </head>\n",
    "#     <body>\n",
    "#         <div1>\n",
    "#            <a href=\"d\">Elsie</a>\n",
    "#            smoothy it\n",
    "#         </div1>\n",
    "#         <p3 class=\"story\">bang</p3>\n",
    "#     </body>\n",
    "#     <footer>\n",
    "#         <div_f>footer is</div_f>\n",
    "#     </footer>\n",
    "#     <navigation>\n",
    "#         <div>heelo</div>\n",
    "#     </navigation>\n",
    "# </html>\n",
    "# \"\"\"\n",
    "# soup = BeautifulSoup(html_file, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full_path1 = 'C:\\\\Users\\\\Office\\\\Desktop\\\\Python Workspace\\\\Tag Classification Project\\\\Dataset\\\\test\\\\test1.html'\n",
    "\n",
    "# html_file = codecs.open(full_path1, 'r')\n",
    "# soup = BeautifulSoup(html_file, 'html.parser')\n",
    "# soup = html_preprocessing(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# def n_of_int(tag):\n",
    "#     count = 0\n",
    "#     tokens = nltk.word_tokenize(tag)\n",
    "    \n",
    "#     for_remove_char1 = []\n",
    "#     for word in tokens:\n",
    "#         for_remove_char1 += word.split('.')\n",
    "#     for_remove_char2 = []\n",
    "#     for word in for_remove_char1:\n",
    "#         for_remove_char2 += word.split('/')\n",
    "#     for_remove_char3 = []\n",
    "#     for word in for_remove_char2:\n",
    "#         for_remove_char3 += word.split(':')   \n",
    "    \n",
    "    \n",
    "#     print for_remove_char3\n",
    "#     for i in for_remove_char3:\n",
    "#         if(i.isdigit()): # 숫자인지 확인\n",
    "#             count = count+1\n",
    "#     return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2016. 10. 28\n",
    "# def find_leaf_node(tag):\n",
    "    \n",
    "#     if tag.string!=None and tag.string!=' ' and len(list(tag.children))==1 and len(list(tag.descendants))==1:\n",
    "#         tagli = extract_nested_level(tag).encode('utf-8')\n",
    "        \n",
    "#         if filtering_tagname(tag, tagli)==1 and filtering_tagattrs(tag.attrs.values())==1: # if pass, continue \n",
    "#             if filtering_tagattrs(tag.parent.attrs.values())==1 and filtering_tagattrs(tag.parent.parent.attrs.values())==1:\n",
    "#                     #if filtering_full_hyperlink(tag)==1:\n",
    "                        \n",
    "#                         #list = [ # of tokens , # of integers, diff, ratio, # of parents, # of siblings, # of sametag ]\n",
    "#                         nonseq_list = [ (labeling_data(tag, publisher)), \n",
    "#                                         (n_of_tok(tag)), (n_of_int(tag)), (is_comma_in_string(tag)), \n",
    "#                                         float(ratio(n_of_int(tag),n_of_tok(tag))), \n",
    "#                                         (len(list(tag.previous_elements))), #(len(list(tag.next_elements))),\n",
    "#                                         (is_date(tag.string)), #(distance_to_h1(tag)), # for date class\n",
    "#                                         (n_of_parents(tag)), (n_pre_sib(tag)), (n_total_sib(tag)), (n_next_sib(tag)),\n",
    "#                                         (is_tag(tag, 'h1')), (is_tag(tag, 'p')), #(is_tag(tag, 'span')), \n",
    "#                                         (is_tag(tag, 'time')), (is_tag(tag, 'div')),\n",
    "\n",
    "#                                         (allli_is_tag(tagli, 'article')), (allli_is_tag(tagli, 'header')),\n",
    "#                                         (allli_is_tag(tagli, 'section')), (allli_is_tag(tagli, 'li')), #(allli_is_tag(tagli, 'ol')),\n",
    "#                                         (allli_is_tag(tagli, 'ui')), \n",
    "#                                         (allli_is_tag(tagli, 'figure')), \n",
    "#                                         (allli_is_tag(tagli, 'select')), (allli_is_tag(tagli, 'form')),\n",
    "#                                         #(allli_is_tag(tagli, 'h2')),(allli_is_tag(tagli, 'h3')),(allli_is_tag(tagli, 'h4')), \n",
    "#                                         (allli_is_tag(tagli, 'p'))\n",
    "#                                       ]\n",
    "\n",
    "#                         leafs_nseq_info.append(nonseq_list)\n",
    "\n",
    "\n",
    "#                         #seq_list = [ labeling_data(tag), extract_tag_name_attrs(tag) ]\n",
    "#                         temp_list = []\n",
    "#                         temp_list.append(labeling_data(tag, publisher))\n",
    "#                         seq_list = temp_list + extract_tag_name_attrs(tag)\n",
    "#                         leafs_seq_info.append(seq_list)\n",
    "\n",
    "#                         leafs_nested_level.append(extract_nested_level(tag).encode('utf-8')) # 'ascii' codec can't decode byte 0xc2 in position 23: ordinal not in range(128) 에러 발생 -> utf-8로 encoding\n",
    "#                         return tag"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
