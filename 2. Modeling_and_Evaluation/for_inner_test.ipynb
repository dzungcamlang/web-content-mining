{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct 15 13:59:06 2016\n",
    "\n",
    "@author: Office\n",
    "\"\"\"\n",
    "import os, csv\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.utils import shuffle\n",
    "from data_manipulation_library import *\n",
    "from learning_model_modules import *\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "global labeled_dataset\n",
    "labeled_dataset = []\n",
    "number_of_file = 0\n",
    "\n",
    "### Parameter Setting\n",
    "length_features = 95 # normal index\n",
    "end_point_for_feature_scaling = 87 # array index \n",
    "num_of_test_sample = 3\n",
    "\n",
    "sample_percent = 0.1\n",
    "sample_token = 1\n",
    "times = 10\n",
    "\n",
    "full_path = 'C:\\\\Users\\\\Office\\\\Documents\\\\Python Workspace\\\\Tag Classification Project\\\\'\n",
    "dataset_path = full_path+'Dataset\\\\data_labeled'\n",
    "#data_path = full_path+'Dataset\\\\data_labeled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 그냥 outer test,  No sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# c0p_list = []\n",
    "# c0r_list = []\n",
    "# c0f_list = []\n",
    "# c0s_list = []\n",
    "\n",
    "# c1p_list = []\n",
    "# c1r_list = []\n",
    "# c1f_list = []\n",
    "# c1s_list = []\n",
    "\n",
    "# c2p_list = []\n",
    "# c2r_list = []\n",
    "# c2f_list = []\n",
    "# c2s_list = []\n",
    "\n",
    "# c3p_list = []\n",
    "# c3r_list = []\n",
    "# c3f_list = []\n",
    "# c3s_list = []\n",
    "\n",
    "# for i in range(0, times):\n",
    "#     train, test = load_data(dataset_path, num_of_test_sample)\n",
    "#     X_train, Y_train, test_X, test_y = no_sampling_data(train, test, length_features, end_point_for_feature_scaling)\n",
    "    \n",
    "#     # Train the model \n",
    "#     clf2 = SVC()\n",
    "#     clf2.fit(X_train, Y_train) \n",
    "    \n",
    "#     # Predict the model\n",
    "#     prediction_SVM = clf2.predict(test_X)\n",
    "    \n",
    "#     print len(train), len(test), len(X_train), len(Y_train), len(test_X), len(test_y), len(prediction_SVM)\n",
    "    \n",
    "#     target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "#     a = classification_report(test_y, prediction_SVM, target_names=target_names)\n",
    "#     print(a)\n",
    "#     print '\\n\\n'\n",
    "    \n",
    "#     c0p = a[72]+a[73]+a[74]+a[75]\n",
    "#     c0r = a[82]+a[83]+a[84]+a[85]\n",
    "#     c0f = a[92]+a[93]+a[94]+a[95]\n",
    "#     c0s = a[101]+a[102]+a[103]+a[104]+a[105]\n",
    "\n",
    "#     c1p = a[125]+a[126]+a[127]+a[128]\n",
    "#     c1r = a[135]+a[136]+a[137]+a[138]\n",
    "#     c1f = a[145]+a[146]+a[147]+a[148]\n",
    "#     c1s = a[154]+a[155]+a[156]+a[157]+a[158]\n",
    "\n",
    "#     c2p = a[178]+a[179]+a[180]+a[181]\n",
    "#     c2r = a[188]+a[189]+a[190]+a[191]\n",
    "#     c2f = a[198]+a[199]+a[200]+a[201]\n",
    "#     c2s = a[207]+a[208]+a[209]+a[210]+a[211]\n",
    "\n",
    "#     c3p = a[231]+a[232]+a[233]+a[234]\n",
    "#     c3r = a[241]+a[242]+a[243]+a[244]\n",
    "#     c3f = a[251]+a[252]+a[253]+a[254]\n",
    "#     c3s = a[260]+a[261]+a[262]+a[263]+a[264]\n",
    "    \n",
    "#     c0p_list.append(c0p)\n",
    "#     c0r_list.append(c0r)\n",
    "#     c0f_list.append(c0f)\n",
    "#     c0s_list.append(c0s)\n",
    "\n",
    "#     c1p_list.append(c1p)\n",
    "#     c1r_list.append(c1r)\n",
    "#     c1f_list.append(c1f)\n",
    "#     c1s_list.append(c1s)\n",
    "\n",
    "#     c2p_list.append(c2p)\n",
    "#     c2r_list.append(c2r)\n",
    "#     c2f_list.append(c2f)\n",
    "#     c2s_list.append(c2s)\n",
    "\n",
    "#     c3p_list.append(c3p)\n",
    "#     c3r_list.append(c3r)\n",
    "#     c3f_list.append(c3f)\n",
    "#     c3s_list.append(c3s)    \n",
    "    \n",
    "# mean_std_print(c0p_list,c0r_list,c0f_list,c0s_list, c1p_list,c1r_list,c1f_list,c1s_list, c2p_list,c2r_list,c2f_list,c2s_list, c3p_list,c3r_list,c3f_list,c3s_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 언론사 증가 and 데이터 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 개수 고정(sampling x)하고 언론사 개수만 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c0p_list = []\n",
    "# c0r_list = []\n",
    "# c0f_list = []\n",
    "# c0s_list = []\n",
    "\n",
    "# c1p_list = []\n",
    "# c1r_list = []\n",
    "# c1f_list = []\n",
    "# c1s_list = []\n",
    "\n",
    "# c2p_list = []\n",
    "# c2r_list = []\n",
    "# c2f_list = []\n",
    "# c2s_list = []\n",
    "\n",
    "# c3p_list = []\n",
    "# c3r_list = []\n",
    "# c3f_list = []\n",
    "# c3s_list = []\n",
    "\n",
    "# for j in range(0, times): # just 10 times\n",
    "#     n_of_publisher = 1\n",
    "    \n",
    "\n",
    "#     for i in range(1, 11):\n",
    "        \n",
    "#         if n_of_publisher < 4: num_of_test_sample = 1\n",
    "#         if n_of_publisher >= 4 and n_of_publisher <=7: num_of_test_sample = 2\n",
    "#         if n_of_publisher >= 8: num_of_test_sample = 3\n",
    "        \n",
    "#         train, test = load_data_aug_publisher(dataset_path, num_of_test_sample, n_of_publisher)\n",
    "#         X_train, Y_train, test_X, test_y = no_sampling_data(train, test, length_features, end_point_for_feature_scaling)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Train the model \n",
    "#         clf2 = SVC()\n",
    "#         clf2.fit(X_train, Y_train) \n",
    "\n",
    "#         # Predict the model\n",
    "#         prediction_SVM = clf2.predict(test_X)\n",
    "\n",
    "#         print len(train), len(test), len(X_train), len(Y_train), len(test_X), len(test_y), len(prediction_SVM)\n",
    "\n",
    "#         target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "#         a = classification_report(test_y, prediction_SVM, target_names=target_names)\n",
    "#         print(a)\n",
    "\n",
    "#         c0p = a[72]+a[73]+a[74]+a[75]\n",
    "#         c0r = a[82]+a[83]+a[84]+a[85]\n",
    "#         c0f = a[92]+a[93]+a[94]+a[95]\n",
    "#         c0s = a[101]+a[102]+a[103]+a[104]+a[105]\n",
    "\n",
    "#         c1p = a[125]+a[126]+a[127]+a[128]\n",
    "#         c1r = a[135]+a[136]+a[137]+a[138]\n",
    "#         c1f = a[145]+a[146]+a[147]+a[148]\n",
    "#         c1s = a[154]+a[155]+a[156]+a[157]+a[158]\n",
    "\n",
    "#         c2p = a[178]+a[179]+a[180]+a[181]\n",
    "#         c2r = a[188]+a[189]+a[190]+a[191]\n",
    "#         c2f = a[198]+a[199]+a[200]+a[201]\n",
    "#         c2s = a[207]+a[208]+a[209]+a[210]+a[211]\n",
    "\n",
    "#         c3p = a[231]+a[232]+a[233]+a[234]\n",
    "#         c3r = a[241]+a[242]+a[243]+a[244]\n",
    "#         c3f = a[251]+a[252]+a[253]+a[254]\n",
    "#         c3s = a[260]+a[261]+a[262]+a[263]+a[264]\n",
    "\n",
    "#         c0p_list.append(c0p)\n",
    "#         c0r_list.append(c0r)\n",
    "#         c0f_list.append(c0f)\n",
    "#         c0s_list.append(c0s)\n",
    "\n",
    "#         c1p_list.append(c1p)\n",
    "#         c1r_list.append(c1r)\n",
    "#         c1f_list.append(c1f)\n",
    "#         c1s_list.append(c1s)\n",
    "\n",
    "#         c2p_list.append(c2p)\n",
    "#         c2r_list.append(c2r)\n",
    "#         c2f_list.append(c2f)\n",
    "#         c2s_list.append(c2s)\n",
    "\n",
    "#         c3p_list.append(c3p)\n",
    "#         c3r_list.append(c3r)\n",
    "#         c3f_list.append(c3f)\n",
    "#         c3s_list.append(c3s)    \n",
    "        \n",
    "        \n",
    "#         print '\\n'\n",
    "        \n",
    "#         n_of_publisher += 1\n",
    "#         print i, n_of_publisher\n",
    "        \n",
    "        \n",
    "#     print '\\n\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터만 증가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling per class (언론사 고정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0, times):\n",
    "#     train, test = load_data(dataset_path, num_of_test_sample)\n",
    "\n",
    "#     gap = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9999999]\n",
    "#     for sample_percent in gap:\n",
    "#         X_train, Y_train, test_X, test_y = sampling_data_class(train, test, sample_percent, length_features, end_point_for_feature_scaling)\n",
    "#         SVM(X_train, Y_train, test_X, test_y)\n",
    "    \n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling per total (언론사 고정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0, times):\n",
    "#     train, test = load_data(dataset_path, num_of_test_sample)\n",
    "\n",
    "#     gap = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9999999]\n",
    "#     for sample_percent in gap:\n",
    "#         X_train, Y_train, test_X, test_y = sampling_data_total(train, test, sample_percent, length_features, end_point_for_feature_scaling)\n",
    "#         SVM(X_train, Y_train, test_X, test_y)\n",
    "    \n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling per class (언론사 랜덤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(0, times):\n",
    "\n",
    "#     gap = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9999999]\n",
    "#     for sample_percent in gap:\n",
    "#         train, test = load_data(dataset_path, num_of_test_sample)\n",
    "#         X_train, Y_train, test_X, test_y = sampling_data_class(train, test, sample_percent, length_features, end_point_for_feature_scaling)\n",
    "#         SVM(X_train, Y_train, test_X, test_y)\n",
    "    \n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sampling per total (언론사 랜덤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# times = 10\n",
    "# for i in range(0, times):\n",
    "\n",
    "#     gap = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9999999]\n",
    "#     for sample_percent in gap:\n",
    "#         train, test = load_data(dataset_path, num_of_test_sample)\n",
    "#         X_train, Y_train, test_X, test_y = sampling_data_total(train, test, sample_percent, length_features, end_point_for_feature_scaling)\n",
    "#         SVM(X_train, Y_train, test_X, test_y)\n",
    "    \n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For outer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Sample Process \n",
    "# group_of_items = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] \n",
    "# num_to_select = num_of_test_sample # test set을 구성할 언론사 폴더 개수                          \n",
    "# list_of_random_items = random.sample(group_of_items, num_to_select)\n",
    "# random_list = [0]*len(group_of_items)\n",
    "\n",
    "# for j, ele in enumerate(list_of_random_items):\n",
    "#     for i, num in enumerate(group_of_items):\n",
    "#         if num==ele:\n",
    "#             random_list[i]=1\n",
    "    \n",
    "# # ex. random_list = [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# #print random_list\n",
    "\n",
    "# labeled_train_dataset = sample_csv_data_load_from_multiple_folders(dataset_path, labeled_dataset, random_list, 1)\n",
    "# labeled_test_dataset = sample_csv_data_load_from_multiple_folders(dataset_path, labeled_dataset, random_list, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling possible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Data Load\n",
    "\n",
    "# #### 왜 이런지 모르겠음\n",
    "# # 혹시 필요할지도.. 20 length가 아니면 모두 filtering\n",
    "# labeled_train_dataset = [row for row in labeled_train_dataset if len(row)==length_features]\n",
    "# labeled_test_dataset = [row for row in labeled_test_dataset if len(row)==length_features]\n",
    "# # '[4' 이런식으로 찍히는거 '4'로 변환\n",
    "# for row in labeled_train_dataset:\n",
    "#     if list(row[0])[0]=='[': \n",
    "#         row[0] = list(row[0])[1]\n",
    "# for row in labeled_test_dataset:\n",
    "#     if list(row[0])[0]=='[': \n",
    "#         row[0] = list(row[0])[1]\n",
    "\n",
    "# print len(labeled_train_dataset)\n",
    "# print len(labeled_test_dataset)\n",
    "\n",
    "# ### Filter dataset based on class\n",
    "# train_class_1_dataset = [row for row in labeled_train_dataset if '1'==row[0] or '[1'== row[0]]\n",
    "# train_class_2_dataset = [row for row in labeled_train_dataset if '2'==row[0] or '[2'== row[0]]\n",
    "# train_class_3_dataset = [row for row in labeled_train_dataset if '3'==row[0] or '[3'== row[0]]\n",
    "# train_class_4_dataset = [row for row in labeled_train_dataset if '4'==row[0] or '[4'== row[0]]\n",
    "\n",
    "# test_class_1_dataset = [row for row in labeled_test_dataset if '1'==row[0] or '[1'== row[0]]\n",
    "# test_class_2_dataset = [row for row in labeled_test_dataset if '2'==row[0] or '[2'== row[0]]\n",
    "# test_class_3_dataset = [row for row in labeled_test_dataset if '3'==row[0] or '[3'== row[0]]\n",
    "# test_class_4_dataset = [row for row in labeled_test_dataset if '4'==row[0] or '[4'== row[0]]\n",
    "\n",
    "# print '*** labeled_train_dataset ***'\n",
    "# print ('#class1:', len(train_class_1_dataset), '#class2:', len(train_class_2_dataset),\n",
    "#         '#class3:', len(train_class_3_dataset), '#class4:', len(train_class_4_dataset))\n",
    "# nor = len(labeled_train_dataset)\n",
    "# print ('%class1:', len(train_class_1_dataset)/float(nor)*100, '%class2:', len(train_class_2_dataset)/float(nor)*100,\n",
    "#         '%class3:', len(train_class_3_dataset)/float(nor)*100, '%class4:', len(train_class_4_dataset)/float(nor)*100)\n",
    "\n",
    "# print '*** labeled_test_dataset ***'\n",
    "# print ('#class1:', len(test_class_1_dataset), '#class2:', len(test_class_2_dataset),\n",
    "#         '#class3:', len(test_class_3_dataset), '#class4:', len(test_class_4_dataset))\n",
    "# nor = len(labeled_test_dataset)\n",
    "# print ('%class1:', len(test_class_1_dataset)/float(nor)*100, '%class2:', len(test_class_2_dataset)/float(nor)*100,\n",
    "#         '%class3:', len(test_class_3_dataset)/float(nor)*100, '%class4:', len(test_class_4_dataset)/float(nor)*100)\n",
    "\n",
    "\n",
    "# #########\n",
    "# # Sampling\n",
    "\n",
    "# # train_class_1_dataset, class_1_test = train_test_split(train_class_1_dataset, train_size = sample_percent)\n",
    "# # train_class_2_dataset, class_2_test = train_test_split(train_class_2_dataset, train_size = sample_percent)\n",
    "# # train_class_3_dataset, class_3_test = train_test_split(train_class_3_dataset, train_size = sample_percent)\n",
    "# # train_class_4_dataset, class_4_test = train_test_split(train_class_4_dataset, train_size = sample_percent)\n",
    "\n",
    "# # test_class_1_dataset, class_1_test = train_test_split(test_class_1_dataset, train_size = sample_percent)\n",
    "# # test_class_2_dataset, class_2_test = train_test_split(test_class_2_dataset, train_size = sample_percent)\n",
    "# # test_class_3_dataset, class_3_test = train_test_split(test_class_3_dataset, train_size = sample_percent)\n",
    "# # test_class_4_dataset, class_4_test = train_test_split(test_class_4_dataset, train_size = sample_percent)\n",
    "\n",
    "# #########\n",
    "\n",
    "# ### Join dataset\n",
    "# outer_train_dataset = train_class_1_dataset+train_class_2_dataset+train_class_3_dataset+train_class_4_dataset\n",
    "# outer_test_dataset = test_class_1_dataset+test_class_2_dataset+test_class_3_dataset+test_class_4_dataset\n",
    "\n",
    "# ### Shuffle dataset\n",
    "# outer_shuf_train_dataset = shuffle(outer_train_dataset)\n",
    "# outer_shuf_test_dataset = shuffle(outer_test_dataset)\n",
    "\n",
    "\n",
    "# ### Split X and Y\n",
    "# outer_X_train = []\n",
    "# outer_Y_train = []\n",
    "# outer_temp1 = []\n",
    "\n",
    "# outer_X_test = []\n",
    "# outer_Y_test = []\n",
    "# outer_temp = []\n",
    "\n",
    "# for idx, row in enumerate(outer_shuf_train_dataset):\n",
    "#     for j in range(1, end_point_for_feature_scaling+1):\n",
    "#         outer_temp1.append(outer_shuf_train_dataset[idx][j])\n",
    "#     outer_X_train.append(outer_temp1)\n",
    "#     outer_temp1 = [] \n",
    "\n",
    "# for idx, row in enumerate(outer_shuf_train_dataset): outer_Y_train.append(outer_shuf_train_dataset[idx][0])\n",
    "\n",
    "    \n",
    "# for idx, row in enumerate(outer_shuf_test_dataset):\n",
    "#     for j in range(1, end_point_for_feature_scaling+1):\n",
    "#         outer_temp.append(outer_shuf_test_dataset[idx][j])\n",
    "#     outer_X_test.append(outer_temp)\n",
    "#     outer_temp = [] \n",
    "\n",
    "# for idx, row in enumerate(outer_shuf_test_dataset): outer_Y_test.append(outer_shuf_test_dataset[idx][0])\n",
    "    \n",
    "    \n",
    "\n",
    "# ### Convert to numpy array with data-type\n",
    "# X_train = np.array(outer_X_train, dtype='float32')\n",
    "# Y_train = np.array(outer_Y_train, dtype='int64')\n",
    "# outer_X_test = np.array(outer_X_test, dtype='float32')\n",
    "# outer_Y_test = np.array(outer_Y_test, dtype='int64')\n",
    "\n",
    "# # for outer test\n",
    "# test_X = outer_X_test\n",
    "# test_y = outer_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner Test Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.95      1.00      0.97       417\n",
      "    class 1       1.00      0.99      1.00       466\n",
      "    class 2       0.98      0.99      0.98      5874\n",
      "    class 3       0.99      0.98      0.99      7563\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14320\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.94      1.00      0.97       376\n",
      "    class 1       1.00      0.98      0.99       435\n",
      "    class 2       0.98      0.98      0.98      5987\n",
      "    class 3       0.99      0.98      0.98      7522\n",
      "\n",
      "avg / total       0.98      0.98      0.98     14320\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.93      1.00      0.96       395\n",
      "    class 1       1.00      0.99      0.99       480\n",
      "    class 2       0.99      0.98      0.99      6051\n",
      "    class 3       0.99      0.99      0.99      7394\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14320\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.96      1.00      0.98       394\n",
      "    class 1       1.00      0.98      0.99       454\n",
      "    class 2       0.98      0.98      0.98      5910\n",
      "    class 3       0.99      0.98      0.98      7562\n",
      "\n",
      "avg / total       0.98      0.98      0.98     14320\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.96      0.99      0.98       429\n",
      "    class 1       1.00      0.99      0.99       450\n",
      "    class 2       0.98      0.99      0.98      5973\n",
      "    class 3       0.99      0.98      0.98      7469\n",
      "\n",
      "avg / total       0.98      0.98      0.98     14321\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c0p_list = []\n",
    "c0r_list = []\n",
    "c0f_list = []\n",
    "c0s_list = []\n",
    "\n",
    "c1p_list = []\n",
    "c1r_list = []\n",
    "c1f_list = []\n",
    "c1s_list = []\n",
    "\n",
    "c2p_list = []\n",
    "c2r_list = []\n",
    "c2f_list = []\n",
    "c2s_list = []\n",
    "\n",
    "c3p_list = []\n",
    "c3r_list = []\n",
    "c3f_list = []\n",
    "c3s_list = []\n",
    "\n",
    "\n",
    "labeled_dataset = csv_data_load_from_multiple_folders(dataset_path, labeled_dataset, 0)\n",
    "labeled_dataset = [row for row in labeled_dataset if len(row)==length_features]\n",
    "labeled_dataset = shuffle(labeled_dataset)\n",
    "# cross_val_0 = labeled_dataset[0:int(len(labeled_dataset)*0.1)]\n",
    "# cross_val_1 = labeled_dataset[int(len(labeled_dataset)*0.1):int(len(labeled_dataset)*0.2)]\n",
    "# cross_val_2 = labeled_dataset[int(len(labeled_dataset)*0.2):int(len(labeled_dataset)*0.3)]\n",
    "# cross_val_3 = labeled_dataset[int(len(labeled_dataset)*0.3):int(len(labeled_dataset)*0.4)]\n",
    "# cross_val_4 = labeled_dataset[int(len(labeled_dataset)*0.4):int(len(labeled_dataset)*0.5)]\n",
    "# cross_val_5 = labeled_dataset[int(len(labeled_dataset)*0.5):int(len(labeled_dataset)*0.6)]\n",
    "# cross_val_6 = labeled_dataset[int(len(labeled_dataset)*0.6):int(len(labeled_dataset)*0.7)]\n",
    "# cross_val_7 = labeled_dataset[int(len(labeled_dataset)*0.7):int(len(labeled_dataset)*0.8)]\n",
    "# cross_val_8 = labeled_dataset[int(len(labeled_dataset)*0.8):int(len(labeled_dataset)*0.9)]\n",
    "# cross_val_9 = labeled_dataset[int(len(labeled_dataset)*0.9):int(len(labeled_dataset)*1.0)]\n",
    "cross_val_0 = labeled_dataset[0:int(len(labeled_dataset)*0.2)]\n",
    "cross_val_1 = labeled_dataset[int(len(labeled_dataset)*0.2):int(len(labeled_dataset)*0.4)]\n",
    "cross_val_2 = labeled_dataset[int(len(labeled_dataset)*0.4):int(len(labeled_dataset)*0.6)]\n",
    "cross_val_3 = labeled_dataset[int(len(labeled_dataset)*0.6):int(len(labeled_dataset)*0.8)]\n",
    "cross_val_4 = labeled_dataset[int(len(labeled_dataset)*0.8):int(len(labeled_dataset)*1.0)]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "\n",
    "    \n",
    "#     if i==0: \n",
    "#         shuf_test_dataset = cross_val_0\n",
    "#         shuf_train_dataset = cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==1:\n",
    "#         shuf_test_dataset = cross_val_1\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==2:\n",
    "#         shuf_test_dataset = cross_val_2\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==3:\n",
    "#         shuf_test_dataset = cross_val_3\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==4:\n",
    "#         shuf_test_dataset = cross_val_4\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_5+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==5:\n",
    "#         shuf_test_dataset = cross_val_5\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_6+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==6:\n",
    "#         shuf_test_dataset = cross_val_6\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_7+cross_val_8+cross_val_9\n",
    "#     elif i==7:\n",
    "#         shuf_test_dataset = cross_val_7\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_8+cross_val_9\n",
    "#     elif i==8:\n",
    "#         shuf_test_dataset = cross_val_8\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_9\n",
    "#     elif i==9:\n",
    "#         shuf_test_dataset = cross_val_9\n",
    "#         shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3+cross_val_4+cross_val_5+cross_val_6+cross_val_7+cross_val_8\n",
    "\n",
    "\n",
    "    if i==0: \n",
    "        shuf_test_dataset = cross_val_0\n",
    "        shuf_train_dataset = cross_val_1+cross_val_2+cross_val_3+cross_val_4\n",
    "    elif i==1:\n",
    "        shuf_test_dataset = cross_val_1\n",
    "        shuf_train_dataset = cross_val_0+cross_val_2+cross_val_3+cross_val_4\n",
    "    elif i==2:\n",
    "        shuf_test_dataset = cross_val_2\n",
    "        shuf_train_dataset = cross_val_0+cross_val_1+cross_val_3+cross_val_4\n",
    "    elif i==3:\n",
    "        shuf_test_dataset = cross_val_3\n",
    "        shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_4\n",
    "    elif i==4:\n",
    "        shuf_test_dataset = cross_val_4\n",
    "        shuf_train_dataset = cross_val_0+cross_val_1+cross_val_2+cross_val_3\n",
    "\n",
    "\n",
    "\n",
    "    ### Split X and Y\n",
    "    X_train, X_test = [], []\n",
    "    Y_train, Y_test = [], []\n",
    "    temp1, temp2 = [], []\n",
    "\n",
    "    for idx, row in enumerate(shuf_train_dataset):\n",
    "        for j in range(1, end_point_for_feature_scaling+1): # csv file에서 feature가 있는 index (1~13)\n",
    "            temp1.append(shuf_train_dataset[idx][j])\n",
    "        X_train.append(temp1)\n",
    "        temp1 = [] # temp 0으로 초기화\n",
    "\n",
    "    for idx, row in enumerate(shuf_test_dataset):\n",
    "        for j in range(1, end_point_for_feature_scaling+1):\n",
    "            temp2.append(shuf_test_dataset[idx][j])\n",
    "        X_test.append(temp2)\n",
    "        temp2 = [] \n",
    "\n",
    "    for idx, row in enumerate(shuf_train_dataset): Y_train.append(shuf_train_dataset[idx][0])\n",
    "    for idx, row in enumerate(shuf_test_dataset): Y_test.append(shuf_test_dataset[idx][0])\n",
    "\n",
    "    ### Convert to numpy array with data-type\n",
    "    X_train = np.array(X_train, dtype='float32')\n",
    "    X_test = np.array(X_test, dtype='float32')\n",
    "    Y_train = np.array(Y_train, dtype='int64')\n",
    "    Y_test = np.array(Y_test, dtype='int64')\n",
    "\n",
    "    # for inner test\n",
    "    test_X = X_test\n",
    "    test_y = Y_test\n",
    "    \n",
    "    \n",
    "    clf2 = SVC()\n",
    "    clf2.fit(X_train, Y_train) \n",
    "    prediction_SVM = clf2.predict(test_X)\n",
    "    \n",
    "    target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "    a = classification_report(test_y, prediction_SVM, target_names=target_names)\n",
    "    print(a)\n",
    "    print \"\\n\"\n",
    "    \n",
    "    c0p = a[72]+a[73]+a[74]+a[75]\n",
    "    c0r = a[82]+a[83]+a[84]+a[85]\n",
    "    c0f = a[92]+a[93]+a[94]+a[95]\n",
    "    c0s = a[101]+a[102]+a[103]+a[104]+a[105]\n",
    "\n",
    "    c1p = a[125]+a[126]+a[127]+a[128]\n",
    "    c1r = a[135]+a[136]+a[137]+a[138]\n",
    "    c1f = a[145]+a[146]+a[147]+a[148]\n",
    "    c1s = a[154]+a[155]+a[156]+a[157]+a[158]\n",
    "\n",
    "    c2p = a[178]+a[179]+a[180]+a[181]\n",
    "    c2r = a[188]+a[189]+a[190]+a[191]\n",
    "    c2f = a[198]+a[199]+a[200]+a[201]\n",
    "    c2s = a[207]+a[208]+a[209]+a[210]+a[211]\n",
    "\n",
    "    c3p = a[231]+a[232]+a[233]+a[234]\n",
    "    c3r = a[241]+a[242]+a[243]+a[244]\n",
    "    c3f = a[251]+a[252]+a[253]+a[254]\n",
    "    c3s = a[260]+a[261]+a[262]+a[263]+a[264]\n",
    "    \n",
    "    c0p_list.append(c0p)\n",
    "    c0r_list.append(c0r)\n",
    "    c0f_list.append(c0f)\n",
    "    c0s_list.append(c0s)\n",
    "\n",
    "    c1p_list.append(c1p)\n",
    "    c1r_list.append(c1r)\n",
    "    c1f_list.append(c1f)\n",
    "    c1s_list.append(c1s)\n",
    "\n",
    "    c2p_list.append(c2p)\n",
    "    c2r_list.append(c2r)\n",
    "    c2f_list.append(c2f)\n",
    "    c2s_list.append(c2s)\n",
    "\n",
    "    c3p_list.append(c3p)\n",
    "    c3r_list.append(c3r)\n",
    "    c3f_list.append(c3f)\n",
    "    c3s_list.append(c3s)\n",
    "    \n",
    "    labeled_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 94.8 1.16619037897 - 99.8 0.4 - 97.2 0.748331477355 - 40220.0 1866.97616482\n",
      "class 1: 100.0 0.0 - 98.6 0.489897948557 - 99.2 0.4 - 45700.0 1517.89327688\n",
      "class 2: 98.2 0.4 - 98.4 0.489897948557 - 97.2 0.4 - 595900.0 6175.75906266\n",
      "class 3: 99.0 0.0 - 98.2 0.4 - 97.2 0.489897948557 - 750200.0 6399.06243133\n"
     ]
    }
   ],
   "source": [
    "mean_std_print(c0p_list,c0r_list,c0f_list,c0s_list, c1p_list,c1r_list,c1f_list,c1s_list, c2p_list,c2r_list,c2f_list,c2s_list, c3p_list,c3r_list,c3f_list,c3s_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For inner test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Data Load\n",
    "# labeled_dataset = csv_data_load_from_multiple_folders(dataset_path, labeled_dataset, 0)\n",
    "\n",
    "# #### 왜 이런지 모르겠음\n",
    "# # 혹시 필요할지도.. 20 length가 아니면 모두 filtering\n",
    "# labeled_dataset = [row for row in labeled_dataset if len(row)==length_features]\n",
    "# # '[4' 이런식으로 찍히는거 '4'로 변환\n",
    "# for row in labeled_dataset:\n",
    "#     if list(row[0])[0]=='[': \n",
    "#         row[0] = list(row[0])[1]\n",
    "# #####\n",
    "\n",
    "# # Feature scaling\n",
    "# #labeled_dataset = feature_scaling(labeled_dataset, 1, end_point_for_feature_scaling)\n",
    "            \n",
    "# ##### Class Imbalance Problem\n",
    "# # class가 구성이 한쪽으로 치우쳐져 있기 때문에, class별로 나눠서 train/test 균등분리가 필요하다. \n",
    "# # 또는 majority class를 undersampling, minority class를 oversampling하는 방법도 있다.\n",
    "\n",
    "\n",
    "# ### Filter dataset based on class\n",
    "# class_1_dataset = [row for row in labeled_dataset if '1'==row[0] or '[1'== row[0]]\n",
    "# class_2_dataset = [row for row in labeled_dataset if '2'==row[0] or '[2'== row[0]]\n",
    "# class_3_dataset = [row for row in labeled_dataset if '3'==row[0] or '[3'== row[0]]\n",
    "# class_4_dataset = [row for row in labeled_dataset if '4'==row[0] or '[4'== row[0]]\n",
    "\n",
    "# print ('#class1:', len(class_1_dataset), '#class2:', len(class_2_dataset),\n",
    "#         '#class3:', len(class_3_dataset), '#class4:', len(class_4_dataset))\n",
    "# nor = len(labeled_dataset)\n",
    "# print ('%class1:', (len(class_1_dataset)/float(nor))*100 if float(nor)!=0 else 0, \n",
    "#        '%class2:', (len(class_2_dataset)/float(nor))*100 if float(nor)!=0 else 0,\n",
    "#        '%class3:', (len(class_3_dataset)/float(nor))*100 if float(nor)!=0 else 0, \n",
    "#        '%class4:', (len(class_4_dataset)/float(nor))*100 if float(nor)!=0 else 0)\n",
    "\n",
    "# ##### Class Imbalance Problem\n",
    "# # class가 구성이 한쪽으로 치우쳐져 있기 때문에, class별로 나눠서 train/test 균등분리가 필요하다. \n",
    "# # 또는 majority class를 undersampling, minority class를 oversampling하는 방법도 있다.\n",
    "\n",
    "\n",
    "# #########\n",
    "\n",
    "\n",
    "# #########\n",
    "\n",
    "\n",
    "# ### Split training/test dataset based on class\n",
    "# class_1_train, class_1_test = train_test_split(class_1_dataset, train_size = 0.8)\n",
    "# class_2_train, class_2_test = train_test_split(class_2_dataset, train_size = 0.8)\n",
    "# class_3_train, class_3_test = train_test_split(class_3_dataset, train_size = 0.8)\n",
    "# class_4_train, class_4_test = train_test_split(class_4_dataset, train_size = 0.8)\n",
    "\n",
    "# ### Join dataset\n",
    "# train_dataset = class_1_train+class_4_train+class_2_train+class_3_train\n",
    "# test_dataset = class_1_test+class_4_test+class_2_test+class_3_test\n",
    "\n",
    "# ### Shuffle dataset\n",
    "# shuf_train_dataset = shuffle(train_dataset)\n",
    "# shuf_test_dataset = shuffle(test_dataset)\n",
    "\n",
    "# ### Split X and Y\n",
    "# X_train, X_test = [], []\n",
    "# Y_train, Y_test = [], []\n",
    "# temp1, temp2 = [], []\n",
    "\n",
    "# for idx, row in enumerate(shuf_train_dataset):\n",
    "#     for j in range(1, end_point_for_feature_scaling+1): # csv file에서 feature가 있는 index (1~13)\n",
    "#         temp1.append(shuf_train_dataset[idx][j])\n",
    "#     X_train.append(temp1)\n",
    "#     temp1 = [] # temp 0으로 초기화\n",
    "\n",
    "# for idx, row in enumerate(shuf_test_dataset):\n",
    "#     for j in range(1, end_point_for_feature_scaling+1):\n",
    "#         temp2.append(shuf_test_dataset[idx][j])\n",
    "#     X_test.append(temp2)\n",
    "#     temp2 = [] \n",
    "\n",
    "# for idx, row in enumerate(shuf_train_dataset): Y_train.append(shuf_train_dataset[idx][0])\n",
    "# for idx, row in enumerate(shuf_test_dataset): Y_test.append(shuf_test_dataset[idx][0])\n",
    "    \n",
    "# ### Convert to numpy array with data-type\n",
    "# X_train = np.array(X_train, dtype='float32')\n",
    "# X_test = np.array(X_test, dtype='float32')\n",
    "# Y_train = np.array(Y_train, dtype='int64')\n",
    "# Y_test = np.array(Y_test, dtype='int64')\n",
    "\n",
    "# # for inner test\n",
    "# test_X = X_test\n",
    "# test_y = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\" SVM \"\"\"\n",
    "# clf2 = SVC()\n",
    "# clf2.fit(X_train, Y_train) \n",
    "# prediction_SVM = clf2.predict(test_X)\n",
    "# accuracy_score(test_y, prediction_SVM)\n",
    "# confusion_matrix(test_y, prediction_SVM)\n",
    "# target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "# print(classification_report(test_y, prediction_SVM, target_names=target_names))\n",
    "# a = classification_report(test_y, prediction_SVM, target_names=target_names)\n",
    "# for i, j in enumerate(a):\n",
    "#     print i, j\n",
    "# c1p_list = []\n",
    "# c0p = a[72]+a[73]+a[74]+a[75]\n",
    "# c0r = a[82]+a[83]+a[84]+a[85]\n",
    "# c0f = a[92]+a[93]+a[94]+a[95]\n",
    "# c0s = a[101]+a[102]+a[103]+a[104]+a[105]\n",
    "\n",
    "# c1p = a[125]+a[126]+a[127]+a[128]\n",
    "# c1r = a[135]+a[136]+a[137]+a[138]\n",
    "# c1f = a[145]+a[146]+a[147]+a[148]\n",
    "# c1s = a[154]+a[155]+a[156]+a[157]+a[158]\n",
    "\n",
    "# c2p = a[178]+a[179]+a[180]+a[181]\n",
    "# c2r = a[188]+a[189]+a[190]+a[191]\n",
    "# c2f = a[198]+a[199]+a[200]+a[201]\n",
    "# c2s = a[207]+a[208]+a[209]+a[210]+a[211]\n",
    "\n",
    "# c2p = a[231]+a[232]+a[233]+a[234]\n",
    "# c2r = a[241]+a[242]+a[243]+a[244]\n",
    "# c2f = a[251]+a[252]+a[253]+a[254]\n",
    "# c2s = a[260]+a[261]+a[262]+a[263]+a[264]\n",
    "# c1p_list.append(a[72]+a[73]+a[74]+a[75])\n",
    "# print(c2p)\n",
    "# a = float(c1p_list[1])\n",
    "# print(a)\n",
    "\n",
    "# \"\"\" Multi-layer Neural Network \"\"\"\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 2), random_state=1)\n",
    "# clf.fit(X_train, Y_train)\n",
    "# prediction_MNN = clf.predict(test_X)\n",
    "# accuracy_score(test_y, prediction_MNN)\n",
    "# confusion_matrix(test_y, prediction_MNN)\n",
    "# target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "# print(classification_report(test_y, prediction_MNN, target_names=target_names))\n",
    "# metrics.precision_score(test_y, prediction_MNN, average='macro')\n",
    "# metrics.recall_score(test_y, prediction_MNN, average='micro')\n",
    "# metrics.f1_score(test_y, prediction_MNN, average='weighted')\n",
    "# ok=0\n",
    "# one, two, thr, fou = 0, 0, 0, 0\n",
    "# a, b, c, d = 0, 0, 0, 0\n",
    "# for v in test_y:\n",
    "#     if v==1: a+=1\n",
    "#     elif v==2: b+=1\n",
    "#     elif v==3: c+=1\n",
    "#     else: d+=1\n",
    "\n",
    "\n",
    "# for i, v in enumerate(test_y):\n",
    "#     if v==prediction_MNN[i]: \n",
    "#         ok+=1\n",
    "#         if v==1: one+=1\n",
    "#         elif v==2: two+=1\n",
    "#         elif v==3: thr+=1\n",
    "#         else: fou+=1\n",
    "        \n",
    "# total_num = len(prediction_MNN)\n",
    "# accuracy = ok//total_num\n",
    "# print 'total accuracy:', ok/float(total_num)*100.0\n",
    "# print 'class1:',one/float(a)*100.0, '  class2:',two/float(b)*100.0, '  class3:',thr/float(c)*100.0, '  class4:',fou/float(d)*100.0\n",
    "\n",
    "\n",
    "# \"\"\" Random Forest \"\"\"\n",
    "# clf4 = RandomForestClassifier(n_estimators=10)\n",
    "# clf4 = clf.fit(X_train, Y_train)\n",
    "# prediction_RF = clf4.predict(test_X)\n",
    "# ok=0\n",
    "# one, two, thr, fou = 0, 0, 0, 0\n",
    "# a, b, c, d = 0, 0, 0, 0\n",
    "\n",
    "# for v in test_y:\n",
    "#     if v==1: a+=1\n",
    "#     elif v==2: b+=1\n",
    "#     elif v==3: c+=1\n",
    "#     else: d+=1\n",
    "\n",
    "\n",
    "# for i, v in enumerate(test_y):\n",
    "#     if v==prediction_RF[i]: \n",
    "#         ok+=1\n",
    "#         if v==1: one+=1\n",
    "#         elif v==2: two+=1\n",
    "#         elif v==3: thr+=1\n",
    "#         else: fou+=1\n",
    "  \n",
    "# total_num = len(prediction_RF)\n",
    "# accuracy = ok//total_num\n",
    "# print 'total accuracy:', ok/float(total_num)*100.0\n",
    "# print 'class1:',one/float(a)*100.0, '  class2:',two/float(b)*100.0, '  class3:',thr/float(c)*100.0, '  class4:',fou/float(d)*100.0\n",
    "\n",
    "\n",
    "# \"\"\" Logistic regression \"\"\"\n",
    "# clf3 = LogisticRegression()\n",
    "# clf3.fit(X_train, Y_train)\n",
    "# prediction_LR = clf3.predict(test_X)\n",
    "# ok=0\n",
    "# one, two, thr, fou = 0, 0, 0, 0\n",
    "# a, b, c, d = 0, 0, 0, 0\n",
    "\n",
    "# for v in test_y:\n",
    "#     if v==1: a+=1\n",
    "#     elif v==2: b+=1\n",
    "#     elif v==3: c+=1\n",
    "#     else: d+=1\n",
    "\n",
    "\n",
    "# for i, v in enumerate(test_y):\n",
    "#     if v==prediction_LR[i]: \n",
    "#         ok+=1\n",
    "#         if v==1: one+=1\n",
    "#         elif v==2: two+=1\n",
    "#         elif v==3: thr+=1\n",
    "#         else: fou+=1\n",
    "  \n",
    "# total_num = len(prediction_LR)\n",
    "# accuracy = ok//total_num\n",
    "# print 'total accuracy:', ok/float(total_num)*100.0\n",
    "# print 'class1:',one/float(a)*100.0, '  class2:',two/float(b)*100.0, '  class3:',thr/float(c)*100.0, '  class4:',fou/float(d)*100.0\n",
    "\n",
    "\n",
    "# \"\"\" KNN \"\"\" \n",
    "# # Merge data\n",
    "# training_data = np.hstack((X_train, np.atleast_2d(Y_train).T))\n",
    "# testing_data = np.hstack((X_test, np.atleast_2d(Y_test).T))\n",
    "\n",
    "# import math\n",
    "# import operator\n",
    "\n",
    "# def euclideanDistance(instance1, instance2, length):\n",
    "# \tdistance = 0\n",
    "# \tfor x in range(length):\n",
    "# \t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
    "# \treturn math.sqrt(distance)\n",
    " \n",
    "# def getNeighbors(trainingSet, testInstance, k):\n",
    "# \tdistances = []\n",
    "# \tlength = len(testInstance)-1\n",
    "# \tfor x in range(len(trainingSet)):\n",
    "# \t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "# \t\tdistances.append((trainingSet[x], dist))\n",
    "# \tdistances.sort(key=operator.itemgetter(1))\n",
    "# \tneighbors = []\n",
    "# \tfor x in range(k):\n",
    "# \t\tneighbors.append(distances[x][0])\n",
    "# \treturn neighbors\n",
    " \n",
    "# def getResponse(neighbors):\n",
    "# \tclassVotes = {}\n",
    "# \tfor x in range(len(neighbors)):\n",
    "# \t\tresponse = neighbors[x][-1]\n",
    "# \t\tif response in classVotes:\n",
    "# \t\t\tclassVotes[response] += 1\n",
    "# \t\telse:\n",
    "# \t\t\tclassVotes[response] = 1\n",
    "# \tsortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "# \treturn sortedVotes[0][0]\n",
    " \n",
    "# def getAccuracy(testSet, predictions):\n",
    "# \tcorrect = 0\n",
    "# \tfor x in range(len(testSet)):\n",
    "# \t\tif testSet[x][-1] == predictions[x]:\n",
    "# \t\t\tcorrect += 1\n",
    "# \treturn (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "# # generate predictions\n",
    "# predictions=[]\n",
    "# k = 3\n",
    "# for x in range(len(testing_data)):\n",
    "# \tneighbors = getNeighbors(training_data, testing_data[x], k)\n",
    "# \tresult = getResponse(neighbors)\n",
    "# \tpredictions.append(result)\n",
    "# \t#print('> predicted=' + repr(result) + ', actual=' + repr(testing_data[x][-1]))\n",
    "# accuracy = getAccuracy(testing_data, predictions)\n",
    "# print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
